\documentclass[10pt, a4paper]{article}

\input{includes}

\title{Rapport}
\author{GOEDEFROIT Charles}
\subject{Sur l'article : Arbitration Policies for On-Demand User-Level I/O Forwarding on HPC Platforms}
\keywords{}
\date{\today}

\input{configs}

\begin{document}

\begin{titlepage}
	\centering
  \ {} % important
	\vfill
	\vspace{1cm}
	{\scshape\Huge\MyTitle\par}
	\vspace{0.5cm}
	{\Large\MySubject\par}
	\vspace{1cm}
	\MyAuthor
	\vfill
	{\large\MyDate\par}
\end{titlepage}

\newpage


\section{L'objectif de l'article}
% TODO: the objective of the paper;

L'article s'intéresse à différante politique d'accés au noeux de données en fonction des
patternes d'accès au données par l'applicaiton.

L'article propose une politique basé sur le problème du sac à dos (Knapsack problem) à choix
multiple. Ce problem cherche à maximisé la bandpassat global en donnant plus de noeux I/O au
applications qui en on le plus besoin.

Il propose aussi une solution qui permet d'utilisé, à la demande, au niveaux utilisateur et
pandent l'execution, différantes politiques de \emph{I/O forwarding}.

Il montre que leur technique augment de façon transparent l'utilisation de la bande passante
globale jusque'à 85\% par rapport a la politique statique utilisé par défault. Il le montre
avec de nombreuse expérimentations et directement sur une infrastructure (live setup).


\section{Le context de l'article}
% - its scientific context or the background to understand the work;

Le context de l'article est :
- Les applications HPC font de plus en plus d'accès au données nommé (I/O)
- Il y a pliens de type d'applicaitons (accès hétérogénéité aux données, IA, bigData)
- on augment le nombre de moeux de calculs pour avoir plus de performance mais le PFS ne peut pas scale pour autant de noeux. (je suppose une histoire de broadcast qui faut mettre en multicast?)
- le \emph{I|O forwarding} cherche a corriger le problème en diminuent le nombre de machine qui accéde au PFS.


La technique habituelle est d'assigné statiquement un noeux de stockage à un noeux de calcule.
Ces liens ce correspond pas tous le temps au transfére de données d'on l'appication à besoin.
Cette technique amméne à une mauvaise utilisation des resources.

% NInt : noeux interface
% Ncal : noeux de calcule

Le \emph{I|O forwarding} consiste à une groupe de noeux qui resoive les requétes de
l'applicaiton et les transmet au system de fichier paralléle. Permet aussi de changer la form
des requétes (Ex : au lieu de plusieurs petite requétes on en fait une seul grand).

Le \emph{I|O forwarding} :
- une technique très utilisé dans le HPC
- Cette technique permet d'augmenté les performances
- évite les accès direct au machine de stockage parallèle
- NInt ce met entre les noeux de calcule et les noeux PFS
- Ajouté des Nint permet de controlé les demand d'accès au données et d'appliqué des théchniques d'optimisation
- c'est transparent pour les utilisateurs (app, FS...)
- diminuent la contention à accès au serveurs de stockage de données.
- try to répatition uniform ?
- utilisé par des machine du top 500
- 


les théchniques d'optimisation :
- la planification des demandes
- l'agrégation des demandes.
-


Le papier parle de :
- regarde plusieurs politique de répartition des resources en fonction des patternes d'accées au données.
- comparé différant patterns d'accées pour mieux voir les benefices et désavantages
- 

Contributions :
1. Le papier évaluons plusieurs politiques d'allocation \emph{I/O forwarding} => démontre que l'allocation dynamique peut amélioré l'utilisation de la bande passante et l'utilisation des noeux I/O.

2. Une politique d'allocation basé sur \emph{Multiple-Choice Knapsack Problem} (\textbf{MCKP}) pour la réprtition des noeux I/O aux app.

3. Il propose un service \emph{I/O forwarding} appelé \emph{GekkoFWD} qui implémente la politique \textbf{MCKP}. on-demand, au user-level, sans modif du code, simple à déployé, utilise sont propre fs.
Ce fs est enrichi pour perpètre différant déploiement??.

Le papier plaide pour :
- allocation dynamique des noeux I/O par rapport au type de calculs / patternes des applicaiton
- repartire des applications sur plusieur reources pour maximisé l'utilisation de la bande passante.
- Le changement de politique ce fait au démarrage ou au changement des applications (job) lancé. (pour déterminé le mailleur nb de I/O forwarding nodes)
- 

1. maximize la band passante en donnant des noeux de stockage au application qui vont mieux les utilisé. (et pas au applications qui accéde peut au données?).

propose : (user-level + portable + don't edit apps => solution utile)
- user-level i/o solution
- on demand
- applique différante politique d'allocation a l'execution (dynamique)
- changer le nombre de noeux I/O à l'execution sans perturbé l'app
-

\section{The problem}
% - a vision of the problem being treated (types of tasks and resources, objective function, constraints, etc), if possible;


Le pb :
- la connection entre les NInt et les Ncal est statique
- Donc l'application peut utilisé qu'un nombre prédéfinis de noeux I/O est communique avec un seul NInt.
- pas assé flexible
- ça peut amené a de mauvaise allocation des resources (Yu et al. [8] Bez et al. [9])
- La bande passant est déterminé par le fonctionnement des applications (plus ou moins de calcule avec plus ou moins d'accés au données...) ce qui fait qu'avec un nombre statique de noeux ou peut ce retouvé à avoir top de noeux I/O et donc une bande passant pas utilisé a sont pliens potenciel. (caractéristiques de la charge de travail / accès I/O).
- 

- pas facile de changer la strategies sans avoir un mauvé impacte car statique

Il on en premier regardé que toutes les applications n'on pas les même perf avec le même nombre de I/O nodes. Il on vue que certain patternes on besoin de plus de I/O nodes et d'autre moins. Donc il on montré qu'une allocation statique c'est null.

Pour un ensemble de tâches à exécuter et un nombre fixe de de nœuds I/O il faut déterminé le nombre de noeux I/O forwarding qui maximise l'utilisation de la bande passante globale. Ce probléme dois étre considered as an optimization problem.

MCKP :
- un problème d'optimisation
- derived from the 0-1 Knapsack
- items divide in $k$ classes of $N_i$ items
($N$ = ensemble des nombre d'items par classes)
$Items = {a; b; c; d; e; f; g}, k <- 2, Items/k = C <- {{a; b; c}; {d; e; f; g}},$
$card(C_1) = 3 = N_1, card(C_2) = 3 = N_2$
- $x_{ij} = 1$ ssi item $j$ is chosen in class $N_i$
- $w_i$ is the weight for the iéme items that represent number of I/O nodes.
- $W$ the total number of I/O nodes.
- $p_i$ is the bandwidth.

- problem is NP-hard
- la solution es obtenu par "Dynamic Programming" qui a une complexité en temps pseudo-polynomial ($O(W\sum^{k}_{i=1}N_i)$)


compute nodes must be derived by the number of I/O nodes.
on est limité par le nombre total de noeux I/O.

Il on fait des traces pour connaitre les access patterns (total data volum, nb processes makeing I/O requests...)

Il on fait des petit benchmark pour reproduire les patternes et cfaire des teste de perfs.

Pas besoin de profiling car on regarde juste les politique d'allocation.

Ils executé 189 patterns sur la machine MN4.
Ils on fait des groupe de 16 pour simulé chaque politique.

1 pattern = one app for this test.

Il on généré 10 000 groupe de 16 pattern qui prènne le même temps d'execution.
Jusque à 128 I/O forwarding nodes. (128 / 16app = 8 by app)
Number of compute nodes : median 256, minimum 88, maximum 512.

La band passante de chaque groupe est calculé par la somme de toutes les (WriteSize + ReadSize / runningTime)
\begin{equation*}aggregate\ BW=\sum_{a=1}^{16}\left(\frac{W_{a}+R_{a}}{runtime_{a}}\right) \tag{2}\end{equation*}

\section{Explication des algo}
% - an explanation of the algorithms;
Gekkofwd :
- politique d'allocation de noeux I/O.
- 

GekkoFS :
- Un system de fichier local au noeux de calcule
- Un \emph{burst-buffer} qui est un buffer qui ce met entre l'app et le FS pour amélioré les perfs. Ce gain de perf est obtenu car il augmente l'utilisation de la band passante (en permettant le stockage de données en même temps que le calcule) et permet d'attenué la charge sur le PFS quant il y à un peaks de demande.
- Fourni un system de nom qui est partagé entre tous les noeux de calcule. Ce mecanisme peut être remplacé par un PFS (grâce au mode d'extension GekkoFWD).
- 

Grâce à GekkoFWD, GekkoFS est utilisé comme un noeux intermédiaires entre le noeux de calcule et le noeux I/O.

Pour capturé les requêtes de l'app au PFS de façon transparent GekkoFS utilise intércepte les appéles system.

Une fois les requêtes capturé GekkoFWD transfére les requêtes à un seul serveur qui lui vas appliqué la politique d'allocation voulue.

Grâce à GekkoFS ils on donc ajouté un thread qui check si le mappage à changer.

Ce système transparent permet de d'ajouté des optiminsation comme la planification des demandes au niveau des fichiers donc il on ajouté la bibliothèque AGIOS dans GekkoFWD.

AGIOS :
- fourni plusieurs ordonanceur.

AGIOS permet à GekkoFWD d'utilisé différant ordonanceur.

Quand un noeux I/O resoive les requêtes il les envoi à AGIOS pour ordonancé le moment ou elle seront traité.

Une fois fini tous est envoyer au PFS.

\section{Le perfs}
% - commentary regarding the performance evaluation, if present;

Il on un gain global d'utilisation de la band passante jusqu'à 85\% par rapport à la politique statique de base.

Pour les perfs du début ils on :
- fait plusieurs lancement
- exec sur MareNostrum 4 (MN4) supercomputer
- avec différante configuration de forwarding
- avec différant nombre de noeux I/O
- pour voir différants patterns
- utilise un outil FORGE (I/OForwarding Explorer) pour relancer le profiles I/O des app
- il décrive les machines de MN4
- 

Politiques avec les quelle il ce sont comparer :
- \emph{ZERO and ONE Policies}
- \emph{STATIC Policy}
- \emph{SIZE and PROCESS Policies}
- \emph{ORACLE Policy}


Ils on fait les expérimentations sur la platform Grid 5000 (G5K) avec 2 cluster à Nancy : Grimoire (8 noeux) et Gros (124 noeux). % conf

5 différants app kernels + les micro-benchmark IOR :
1. S3D I/O Kernel
2. MADBench2
3. HACC-IO
4. S3aSim
5. NAS BT-IO

Ils on mesuré la band passant pour chaque applications et il confirm les résultat de FORGE. Ils ont fait ce test avec des nombre de noeux et des nombre de processus différant.

TODO: 5.2. Allocation Decisions


\section{Les autre publications}
% - information about other papers that reference it.

TODO: SECTION 6.

\section{Definition *}

La \emph{contention} : forte tentions sur un serveur (beaucoup de demandes).

\end{document}

% TODO: up to 8 pages
