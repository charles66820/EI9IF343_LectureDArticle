\documentclass[10pt, a4paper]{article}

\input{includes}

\title{Rapport}
\author{GOEDEFROIT Charles}
\subject{Sur l'article : Arbitration Policies for On-Demand User-Level I/O Forwarding on HPC Platforms}
\keywords{}
\date{\today}

\input{configs}

\begin{document}

\begin{titlepage}
	\centering
  \ {} % important
	\vfill
	\vspace{1cm}
	{\scshape\Huge\MyTitle\par}
	\vspace{0.5cm}
	{\Large\MySubject\par}
	\vspace{1cm}
	\MyAuthor
	\vfill
	{\large\MyDate\par}
\end{titlepage}

\newpage


\section{L'objectif de l'article}
% TODO: the objective of the paper;

% truc dynamique

L'objectif de l'article est d'amélioré l'utilisation de la bande passante entre les noeud de calcule et les \emph{noeud des stockage des données}, que j'appailerai \emph{noeud I/O} dans le reste des ce rapport. Plus présisément l'objectif est permettre le changement dynamique des politiques d'accés aux \emph{noeud I/O}. Pour cela l'article propose :
\begin{itemize}
  \item une implémentation qui permet ce changement en fontion des patterns d'accé
  \item ainsi qu'une politique basé sur le problème du sac à dos à choix multiple \emph{Multiple-Choice Knapsack problem}.
\end{itemize}

Il propose aussi une solution qui permet d'utilisé, à la demande, au niveaux utilisateur et
pandent l'execution, différentes politiques de \emph{I/O forwarding}.

Il montre que leur technique augment de façon transparent l'utilisation de la bande passante
globale jusque'à 85\% par rapport a la politique statique utilisé par défault. Il le montre
avec de nombreuse expérimentations et directement sur une infrastructure (live setup).
\section{Le context de l'article}
% - its scientific context or the background to understand the work;

Cette article ce place dans le contexte ou les supercalculateur font de plus en plus d'accès au données. Cette augmentation est du à plusieurs facteurs comme :
\begin{itemize}
  \item l'augmentation de la puissance de calcule
  \item l'augmentation des quantités de données a traité
  \item la variété des type d'applications à accès hétérogénéité aux données, l'intéligence artificiel, le bigData\dots
\end{itemize}
Pour répondre au besoin de pussiance croissante on augmente le nombre de noeud de calcule ce qui à pour effét d'aumenté le nombre de requêtes système de fichier parallèle (\emph{PFS}). Ce système fini par ne plus pouvoir traité autent de requêtes car tous les \emph{noeud I/O} ce retouve saturé. Pour réglé ce problème les \emph{PFS} utilise la technique du \emph{I/O forwarding} qui diminue le nombre de noeud du \emph{PFS} accessible par les noeud de calcule.

\subsection{La technique du \emph{I/O forwarding}}

Cette technique est très utilisé pour le calcul haut performance, notamment par des machine du top 500\cite{}. Elle permet d'augmenté les performances global en diminuent la concentration des requêtes sur les \emph{noeud I/O}. Pour cela elle évite les accès direct aux \emph{noeud I/O} en ce placent entre les noeud de calcule et les \emph{noeud I/O}. Concrètement un groupe de \emph{noeud I/O} recevoir les requêtes et applique des traitement avant de les transmet au bon \emph{noeud I/O}. Ce technique est transparent pour les utilisateurs, elle cherche à appliqué une répartition uniforme entre les noeud et permet le controlle des demandes d'accès au données ce qui permet l'application de technique d'optimisation. Ce techniques peuvent être de différentes nature comme l'agrégation des requêtes (en combinent plusieurs petite requêtes) ou la planification des requêtes.

Habituellement les \emph{noeud I/O} qui s'occupe de la réception des requêtes sont assigné statiquement à un noeud de calcule et celle-ci peut dépendre de la topologie du réseau, du nombre de \emph{noeud I/O}\dots Cette assignation ne correspond pas tous les temps au besoin de l'application qui peut ne pas utilisé toute la bande passant ou être ralentie par celle-ci. L'assignation statiquement amène donc à une mauvaise utilisation des resources. Dans la suite du rapport j'appelerai les \emph{noeud I/O} qui s'occupe de la réception les \emph{noeud I/O Forwarding}.

\section{Le problème traité par l'article}

L'article cherche à corriger les problèmes de l'allocation statique en proposent une allocation dynamique.
Les problèmes de l'allocation statique sont :
\begin{itemize}
  \item Le nombre de \emph{noeud I/O} et prédéfinis et donc ne peut pas changer pendant l'execution ou s'adapté à une application.
  \item Ce n'est pas flexible car la politique d'allocation des \emph{noeud I/O} reste la même tous au long de l'execution.
  \item On peut être amené à de mauvaise allocation des resources (l'article cite les travaux de Yu et al. [8] Bez et al. [9])\cite{}
  \item En fonction de l'application on peut ce retrouvé avec une band passant peut utilisé ou qui ralentie l'application. Ce qui fait qu'on utilise pas assez ou trop de \emph{noeud I/O}.
  \item Ne permet pas le changement facile de la strategies d'allocation sans avoir un mauvais impacte sur les performances.
\end{itemize}

\subsection{Évaluations des politique d'allocations}

Dans l'article ils on commancé par évalué les politiques d'allocation suivant :
\begin{itemize}
  \item \emph{ZERO and ONE Policies} chaque application à 0 ou un \emph{noeud I/O} alloué.
  \item \emph{STATIC Policy} le nombre de \emph{noeud I/O} est déterminé par rapport au nombre de noeud de calcul.
  \item \emph{SIZE and PROCESS Policies} le nombre de \emph{noeud I/O} est répartition proportionnellement entre les application par rapport à leur nombre de noeud de calcul ou leur nombre de processus.
  \item \emph{ORACLE Policy} chaque application ce vois alloué un nombre de \emph{noeud I/O} qui maximise l'utilisation de la band passant. Cette allocation est déterminé par une évaluations des performances.
\end{itemize}

Il on effectué ces évaluations en faisant plusieurs lancements avec différent nombre de \emph{noeud I/O}, différentes politiques d'allocation et avec différentes applications pour avoir différent patterns d'accès au PFS. Ces évaluations on été effectué sur le supercalculateur MareNostrum 4 (MN4) en utilisent un outil appelé FORGE (I/O \textbf{For}wardin\textbf{g E}xplorer) qui permet de re-lancer les profiles I/O des applications.

Pour obtenir les résultats ils ont fait des traces pour connaître les patterns d'accès des application au \emph{PFS}. Ils ont collecté le volume total des données transféré et le nombre de processes qui font des requêtes \emph{I/O}. Avec c'est traces ils on créer des petit benchmark pour reproduire les c'est patterns d'accès et faire des teste de performances.

Ils non pas eu besoin de profiling car ont s'intéresse qu'aux politiques d'allocation.

Ils ont éxécuté 189 patterns sur la machine MN4 et il ont fair des group de 16 patterns pour simulé chaque politique (Un pattern représente une application).
Ils ont donc généré 10 000 groupe de 16 patterns qui prènne le même temps d'execution.
Ils ont utilisé jusqu'à 128 \emph{noeud I/O forwarding}, 8 par application ($128/16=8$). Avec un nombre de noeud de calcule entre 88 et 512 et une median de 256.

La band passante de chaque groupe est calculé par la somme de toutes les (WriteSize + ReadSize / runningTime)
\begin{equation*}aggregate\ BW=\sum_{a=1}^{16}\left(\frac{W_{a}+R_{a}}{runtime_{a}}\right) \tag{2}\end{equation*}

Grâce à cette évaluations on vois qu'il est possible d'amélioré l'utilisation de la band passante.
Ils ont aussi vue que toutes les applications n'on pas les même performances avec le même nombre de \emph{noeud I/O}. Cette différence existe car chaque applications a un patterns d'accès aux \emph{PFS} différent. Ils ont remarqué que certain applications avait des patterns similaire.

\section{Propositions de l'article}

La problématique d'une politique d'allocation peut être repésenté comme suit :
Pour un ensemble de tâches ou job à exécuter et un nombre fixe de \emph{noeud I/O} il faut déterminé le nombre de \emph{noeud I/O Forwarding} qui permet de maximisé l'utilisation de la bande passante globale.
Cette problématique peut être considéré comme un problème d'optimisation.

Pour répondre à cette problématique et permettre l'allocation dynamique l'article porpose une nouvelle politique d'allocation ainsi qu'un service permettant l'allocation dynamique.

\subsection{La politique d'allocaion basé sur \emph{MCKP}}

La politique d'allocation basé sur \textbf{MCKP} (\emph{Multiple-Choice Knapsack Problem}) cherche la mailleur répartition possible des \emph{noeud I/O Forwarding} aux noeud de calculs.
Pour cela cette politique cherche à maximisé la band passante global en donnant plus de \emph{noeud I/O Forwarding} au applications qui en on le plus besoin et moins au autre. Ce problème d'optimisation et dérivé du problème \emph{0-1 Knapsack}.
Pour effectué la maximisation de la band passante l'algorithms vérifie que chaque tâches à bien une taille et que le nombre globale de \emph{noeud I/O Forwarding} calculé ne dépasse pas le nombre de \emph{noeud I/O Forwarding} disponible.

Ce problème fait partie de la classe de complexité en temps \emph{NP-hard} mais une solution peut être obtenu par "Dynamic Programming" qui est en temps \emph{pseudo-polynomial}.

Le nombre de noeud de calcule dois être divisable par le nombre de \emph{noeud I/O Forwarding} pour améliorer l'équilibrage de la charge.

% MCKP :
% - Ce problem cherche à maximisé la bandpassat global en donnant plus de noeud I/O au applications qui en on le plus besoin.
% - un problème d'optimisation
% - derived from the 0-1 Knapsack
% - items divide in $k$ classes of $N_i$ items
% ($N$ = ensemble des nombre d'items par classes)
% $Items = {a; b; c; d; e; f; g}, k <- 2, Items/k = C <- {{a; b; c}; {d; e; f; g}},$
% $card(C_1) = 3 = N_1, card(C_2) = 3 = N_2$
% - $x_{ij} = 1$ ssi item $j$ is chosen in class $N_i$
% - $w_i$ is the weight for the iéme items that represent number of I/O nodes.
% - $W$ the total number of I/O nodes.
% - $p_i$ is the bandwidth.
% - problem is NP-hard
% - la solution es obtenu par "Dynamic Programming" qui a une complexité en temps pseudo-polynomial ($O(W\sum^{k}_{i=1}N_i)$)

\subsection{Le service permettant l'allocation dynamique (\emph{GekkoFWD})}

L'article propose un service \emph{I/O forwarding} appelé \emph{GekkoFWD} qui
implémente la politique \textbf{MCKP}. Ce service permet le changement de politiques d'allocation ainsi que le nombre de \emph{noeud I/O Forwarding} à la demande (dynamiquement) sans perturbé l'application. Il fonction au niveau utilisateur, il ne nécessite pas de modification du code des applications et il est simple à déployé. Cette simplicité en fait une solution utilie.
Ce service est basé sur \emph{GekkoFS} qui est une système de fichier local à un noeud et qui peut ce connecté à la plupart des \emph{PFS} existant.

\subsubsection{\emph{GekkoFS}}

\emph{GekkoFS} est un système de fichiers local qui est exécuter sur les noeud de calcule. Ce système est utilisé comme un \emph{burst-buffer}. Comme sont nom l'indique c'est un buffer qui ce met entre l'application et le \emph{PFS} dans le but d'amélioré la performances des accès \emph{I/O}. Cette amélioration est en partie du à la diminution de la charge sur le \emph{PFS} quant il y a un pics de demande.

Ce système permet aussi l'execution des requêtes \emph{I/O en parallèle} de l'execution du calcule, ce qui permet d'augmente l'utilisation de la band passante.

Il fourni aussi un système de nom qui est partagé entre tous les noeud de calcule. Ce mécanisme peut être remplacé par un \emph{PFS}.

\subsubsection{Application de technique d'optimisation (\emph{AGIOS})}

\emph{GekkoFWD} nous permet d'appliqué des techniques d'optimisation de façon transparente. Pour ce faire ils ont ajouté la bibliothèque \emph{AGIOS} dans \emph{GekkoFWD}. Cette bibliothèque fourni plusieurs algorithms d'ordonnancement.
\emph{GekkoFWD} peut donc utilisé différent ordonnanceurs sur les requêtes. Ceci permet la planification des requêtes au niveau des données.

\section{Algorithme}

Grâce à \emph{GekkoFWD}, \emph{GekkoFS} est utilisé comme un noeud intermédiaires entre les noeud de calcule et les \emph{noeud I/O}. Pour cela \emph{GekkoFS} capture les requêtes que fait l'application au \emph{PFS}. Cette capture est transparent et ce fait en interceptant les appéles système. Une fois les requêtes capturé \emph{GekkoFWD} transfére les requêtes à un seul serveur qui vas les passé à \emph{AGIOS} pour ordonancé le moment ou elle seront traité. Grâce au requêtes il est possible de determiné quelle politique d'allocation appliqué.

Pour savoir quant le mappage à changer on ajoute un thread grâce à \emph{GekkoFS}.

\section{Les performances}

Ils on fait les expérimentations sur la platform Grid 5000 (G5K) avec 2 cluster à Nancy : Grimoire (8 noeud) et Gros (124 noeud).

Ils ont utilisé 5 noyaux d'applications différentes ainsi que les micro-benchmark IOR :
\begin{enumerate}
  \item S3D I/O Kernel : S3D
  \item MADBench2 : MAD
  \item HACC-IO : HACC
  \item S3aSim : SIM
  \item NAS BT-IO : BT-C, BT-D
  \item [IOR] MPI-IO / POSIX : IOR-MPI, POSIX-S, POSIX-L
\end{enumerate}

Ils ont mesuré la band passant pour chaque applications et il confirme les résultat de FORGE. Ils ont fait ce test avec des nombre de noeud et des nombre de processus différent.
Les ensembles de tâches pour testé de changement d'allocation dynamique été composé de BT-C, BT-D, IOR-MPI, POSIX-L, MAD, et S3D.

Ils constate bien que les accès statique sont pas très efficace sauf pour S3D. Même constatation pour l'allocation par taille. Dans c'est 2 cas on vois que par exemple IOR-MPI pourrai être bien mailleurs avec plus de \emph{noeud I/O forwarding}. Ils constate bien un nétte ammélioration avec leur implémentation \emph{MCKP}. Le gain global d'utilisation de la band passante est jusqu'à 85\% par rapport à la politique statique.

Ils ont comparer différents sénario pour montré que leur implémentation fonctionne bien.

\section{Les article qu'il référence}
% - information about other papers that reference it.

Ils existe :/

TODO: SECTION 6.

\section{Check / Add}

TODO:

Le papier parle de :
- regarde plusieurs politique de répartition des resources en fonction des patternes d'accées au données.
- comparé différent patterns d'accées pour mieux voir les benefices et désavantages

Le papier plaide pour :
- allocation dynamique des noeud I/O par rapport au type de calculs / patternes des application
- repartire des applications sur plusieur reources pour maximisé l'utilisation de la bande passante.
- Le changement de politique ce fait au démarrage ou au changement des applications (job) lancé. (pour déterminé le mailleur nb de I/O forwarding nodes)

1. maximize la band passante en donnant des noeud de stockage au application qui vont mieux les utilisé. (et pas au applications qui accéde peut au données?).

% load all bibliographies
\bibliography{biblio}
\nocite{*}

\end{document}

% TODO: up to 8 pages

%L'article s'intéresse à différente politique d'accés au noeud de données en fonction des patternes d'accès au données par l'application.

% L'article propose une politique basé sur le problème du sac à dos (Knapsack problem) à choix multiple. Ce problem cherche à maximisé la bandpassat global en donnant plus de noeud I/O au applications qui en on le plus besoin.

%Le context de l'article est :
%- Les applications HPC font de plus en plus d'accès au données nommé (I/O)
%- Il y a pliens de type d'applications (accès hétérogénéité aux données, IA, bigData)
%- on augment le nombre de moeux de calculs pour avoir plus de performance mais le PFS ne peut pas scale pour autant de noeud. (je suppose une histoire de broadcast qui faut mettre en multicast?)
%- le \emph{I/O forwarding} cherche a corriger le problème en diminuent le nombre de machine qui accéde au PFS.

% NInt : noeud interface
% Ncal : noeud de calcule

% Le \emph{I/O forwarding} consiste à une groupe de noeud qui resoive les requétes de
% l'application et les transmet au system de fichier paralléle. Permet aussi de changer la form
% des requétes (Ex : au lieu de plusieurs petite requétes on en fait une seul grand).

% Le \emph{I/O forwarding} :
% - une technique très utilisé dans le HPC
% - Cette technique permet d'augmenté les performances
% - évite les accès direct au machine de stockage parallèle
% - NInt ce met entre les noeud de calcule et les noeud PFS
% - Ajouté des Nint permet de controlé les demand d'accès au données et d'appliqué des théchniques d'optimisation
% - c'est transparent pour les utilisateurs (app, FS...)
% - diminuent la contention à accès au serveurs de stockage de données.
% - try to répatition uniform ?
% - utilisé par des machine du top 500


% les théchniques d'optimisation :
% - la planification des demandes
% - l'agrégation des demandes.

% La technique habituelle est d'assigné statiquement un noeud de stockage à un noeud de calcule.
% Ces liens ce correspond pas tous le temps au transfére de données d'on l'appication à besoin.

% Le pb :
% - la connection entre les NInt et les Ncal est statique
% - Donc l'application peut utilisé qu'un nombre prédéfinis de noeud I/O est communique avec un seul NInt.
% - pas assé flexible
% - ça peut amené a de mauvaise allocation des resources (Yu et al. [8] Bez et al. [9])
% - La bande passant est déterminé par le fonctionnement des applications (plus ou moins de calcule avec plus ou moins d'accés au données...) ce qui fait qu'avec un nombre statique de noeud ou peut ce retouvé à avoir top de noeud I/O et donc une bande passant pas utilisé a sont pliens potenciel. (caractéristiques de la charge de travail / accès I/O).

% Pour les perfs du début ils on :
% - fait plusieurs lancement
% - exec sur MareNostrum 4 (MN4) supercomputer
% - avec différente configuration de forwarding
% - avec différent nombre de noeud I/O
% - pour voir différents patterns
% - utilise un outil FORGE (I/OForwarding Explorer) pour relancer le profiles I/O des app
% - il décrive les machines de MN4

% Politiques avec les quelle il ce sont comparer :
% - \emph{ZERO and ONE Policies}
% - \emph{STATIC Policy}
% - \emph{SIZE and PROCESS Policies}
% - \emph{ORACLE Policy}


% Pas besoin de profiling car on regarde juste les politique d'allocation.

% Ils executé 189 patterns sur la machine MN4.
% Ils on fait des groupe de 16 pour simulé chaque politique.

% 1 pattern = one app for this test.

% Il on généré 10 000 groupe de 16 pattern qui prènne le même temps d'execution.
% Jusque à 128 I/O forwarding nodes. (128 / 16app = 8 by app)
% Number of compute nodes : median 256, minimum 88, maximum 512.

% La band passante de chaque groupe est calculé par la somme de toutes les (WriteSize + ReadSize / runningTime)
% \begin{equation*}aggregate\ BW=\sum_{a=1}^{16}\left(\frac{W_{a}+R_{a}}{runtime_{a}}\right) \tag{2}\end{equation*}

% - pas facile de changer la strategies sans avoir un mauvé impacte car statique

% Il on en premier regardé que toutes les applications n'on pas les même perf avec le même nombre de I/O nodes. Il on vue que certain patternes on besoin de plus de I/O nodes et d'autre moins. Donc il on montré qu'une allocation statique c'est null.

% Pour un ensemble de tâches à exécuter et un nombre fixe de de nœuds I/O il faut déterminé le nombre de noeud I/O forwarding qui maximise l'utilisation de la bande passante globale. Ce probléme dois étre considered as an optimization problem.

% Contributions :
% 1. Le papier évaluons plusieurs politiques d'allocation \emph{I/O forwarding} => démontre que l'allocation dynamique peut amélioré l'utilisation de la bande passante et l'utilisation des noeud I/O.

% 2. Une politique d'allocation basé sur \emph{Multiple-Choice Knapsack Problem} (\textbf{MCKP}) pour la répartition des noeud I/O aux app.

% compute nodes must be divisible by the number of I/O nodes.
% on est limité par le nombre total de noeud I/O.

% 3. Il propose un service \emph{I/O forwarding} appelé \emph{GekkoFWD} qui implémente la politique \textbf{MCKP}. on-demand, au user-level, sans modif du code, simple à déployé, utilise sont propre fs.
% Ce fs est enrichi pour perpètre différent déploiement??.

% propose : (user-level + portable + don't edit apps => solution utile)
% - user-level i/o solution
% - on demand
% - applique différente politique d'allocation a l'execution (dynamique)
% - changer le nombre de noeud I/O à l'execution sans perturbé l'app


% GekkoFS :
% - Un system de fichier local au noeud de calcule
% - Un \emph{burst-buffer} qui est un buffer qui ce met entre l'app et le FS pour amélioré les perfs. Ce gain de perf est obtenu car il augmente l'utilisation de la band passante (en permettant le stockage de données en même temps que le calcule) et permet d'attenué la charge sur le PFS quant il y à un peaks de demande.
% - Fourni un system de nom qui est partagé entre tous les noeud de calcule. Ce mecanisme peut être remplacé par un PFS (grâce au mode d'extension GekkoFWD).

% Grâce à GekkoFWD, GekkoFS est utilisé comme un noeud intermédiaires entre le noeud de calcule et le noeud I/O.

% Pour capturé les requêtes de l'app au PFS de façon transparent GekkoFS intércepte les appéles system.

% Une fois les requêtes capturé GekkoFWD transfére les requêtes à un seul serveur qui lui vas appliqué la politique d'allocation voulue.

% Grâce à GekkoFS ils on donc ajouté un thread qui check si le mappage à changer.

% Ce système transparent permet de d'ajouté des optimisation comme la planification des demandes au niveau des fichiers donc il on ajouté la bibliothèque AGIOS dans GekkoFWD.

% AGIOS :
% - fourni plusieurs ordonanceur.

% AGIOS permet à GekkoFWD d'utilisé différent ordonanceur.

% Quand un noeud I/O resoive les requêtes il les envoi à AGIOS pour ordonancé le moment ou elle seront traité.

% Une fois fini tous est envoyer au PFS.
