\documentclass[10pt, a4paper]{article}

\input{includes}

\title{Rapport}
\author{GOEDEFROIT Charles}
\subject{Sur l'article : Arbitration Policies for On-Demand User-Level I/O Forwarding on HPC Platforms}
\keywords{}
\date{\today}

\input{configs}

\begin{document}

\begin{titlepage}
	\centering
  \ {} % important
	\vfill
	\vspace{1cm}
	{\scshape\Huge\MyTitle\par}
	\vspace{0.5cm}
	{\Large\MySubject\par}
	\vspace{1cm}
	\MyAuthor
	\vfill
	{\large\MyDate\par}
\end{titlepage}

\newpage


\section{L'objectif de l'article}
% TODO: the objective of the paper;

% truc dynamique

L'objectif de l'article est d'amélioré l'utilisation de la bande passante entre les noeud de calcule et les \emph{noeud des stockage des données}, que j'appailerai \emph{noeud I/O} dans le reste des ce rapport. Plus présisément l'objectif est permettre le changement dynamique des politiques d'accés aux \emph{noeud I/O}. Pour cela l'article propose :
\begin{itemize}
  \item une implémentation qui permet ce changement en fontion des patterns d'accé
  \item ainsi qu'une politique basé sur le problème du sac à dos à choix multiple \emph{Multiple-Choice Knapsack problem}.
\end{itemize}

Il propose aussi une solution qui permet d'utilisé, à la demande, au niveaux utilisateur et
pandent l'execution, différentes politiques de \emph{I/O forwarding}.

Il montre que leur technique augment de façon transparent l'utilisation de la bande passante
globale jusque'à 85\% par rapport a la politique statique utilisé par défault. Il le montre
avec de nombreuse expérimentations et directement sur une infrastructure (live setup).
\section{Le context de l'article}
% - its scientific context or the background to understand the work;

Cette article ce place dans le contexte ou les supercalculateur font de plus en plus d'accès au données. Cette augmentation est du à plusieurs facteurs comme :
\begin{itemize}
  \item l'augmentation de la puissance de calcule
  \item l'augmentation des quantités de données a traité
  \item la variété des type d'applications à accès hétérogénéité aux données, l'intéligence artificiel, le bigData\dots
\end{itemize}
Pour répondre au besoin de pussiance croissante on augmente le nombre de noeud de calcule ce qui à pour effét d'aumenté le nombre de requêtes système de fichier parallèle (\emph{PFS}). Ce système fini par ne plus pouvoir traité autent de requêtes car tous les \emph{noeud I/O} ce retouve saturé. Pour réglé ce problème les \emph{PFS} utilise la technique du \emph{I/O forwarding} qui diminue le nombre de noeud du \emph{PFS} accessible par les noeud de calcule.

\subsection{La technique du \emph{I/O forwarding}}

Cette technique est très utilisé pour le calcul haut performance, notamment par des machine du top 500\cite{}. Elle permet d'augmenté les performances global en diminuent la concentration des requêtes sur les \emph{noeud I/O}. Pour cela elle évite les accès direct aux \emph{noeud I/O} en ce placent entre les noeud de calcule et les \emph{noeud I/O}. Concrètement un groupe de \emph{noeud I/O} recevoir les requêtes et applique des traitement avant de les transmet au bon \emph{noeud I/O}. Ce technique est transparent pour les utilisateurs, elle cherche à appliqué une répartition uniforme entre les noeud et permet le controlle des demandes d'accès au données ce qui permet l'application de technique d'optimisation. Ce techniques peuvent être de différentes nature comme l'agrégation des requêtes (en combinent plusieurs petite requêtes) ou la planification des requêtes.

Habituellement les \emph{noeud I/O} qui s'occupe de la réception des requêtes sont assigné statiquement à un noeud de calcule et celle-ci peut dépendre de la topologie du réseau, du nombre de \emph{noeud I/O}\dots Cette assignation ne correspond pas tous les temps au besoin de l'application qui peut ne pas utilisé toute la bande passant ou être ralentie par celle-ci. L'assignation statiquement amène donc à une mauvaise utilisation des resources.

\section{Le problème traité par l'article}

L'article cherche à corriger les problèmes de l'allocation statique en proposent une allocation dynamique.
Les problèmes de l'allocation statique sont :
\begin{itemize}
  \item Le nombre de \emph{noeud I/O} et prédéfinis et donc ne peut pas changer pendant l'execution ou s'adapté à une application.
  \item Ce n'est pas flexible car la politique d'allocation des \emph{noeud I/O} reste la même tous au long de l'execution.
  \item On peut être amené à de mauvaise allocation des resources (l'article cite les travaux de Yu et al. [8] Bez et al. [9])\cite{}
  \item En fonction de l'application on peut ce retrouvé avec une band passant peut utilisé ou qui ralentie l'application. Ce qui fait qu'on utilise pas assez ou trop de \emph{noeud I/O}.
  \item Ne permet pas le changement facile de la strategies d'allocation sans avoir un mauvais impacte sur les performances.
\end{itemize}

\subsection{Évaluations des politique d'allocations}

Dans l'article ils on commancé par évalué les politiques d'allocation suivant :
\begin{itemize}
  \item \emph{ZERO and ONE Policies} chaque application à 0 ou un \emph{noeud I/O} alloué.
  \item \emph{STATIC Policy} le nombre de \emph{noeud I/O} est déterminé par rapport au nombre de noeud de calcul.
  \item \emph{SIZE and PROCESS Policies} le nombre de \emph{noeud I/O} est répartition proportionnellement entre les application par rapport à leur nombre de noeud de calcul ou leur nombre de processus.
  \item \emph{ORACLE Policy} chaque application ce vois alloué un nombre de \emph{noeud I/O} qui maximise l'utilisation de la band passant. Cette allocation est déterminé par une évaluations des performances.
\end{itemize}

Il on effectué ces évaluations en faisant plusieurs lancements avec différent nombre de \emph{noeud I/O}, différentes politiques d'allocation et avec différentes applications pour avoir différent patterns d'accès au PFS. Ces évaluations on été effectué sur le supercalculateur MareNostrum 4 (MN4) en utilisent un outil appelé FORGE (I/O \textbf{For}wardin\textbf{g E}xplorer) qui petmet de re-lancer les profiles I/O des applications.

Grâce à cette évaluations on vois qu'il est possible d'amélioré l'utilisation de la band passante.
Ils ont aussi vue que toutes les applications n'on pas les même performances avec le même nombre de \emph{noeud I/O}. Cette différence existe car chaque applications a un patterns d'accès aux \emph{PFS} différent. Ils ont remarqué que certain applications avait des patterns similaire.


Le papier parle de :
- regarde plusieurs politique de répartition des resources en fonction des patternes d'accées au données.
- comparé différent patterns d'accées pour mieux voir les benefices et désavantages
- 

Contributions :
% 1. Le papier évaluons plusieurs politiques d'allocation \emph{I/O forwarding} => démontre que l'allocation dynamique peut amélioré l'utilisation de la bande passante et l'utilisation des noeud I/O.

2. Une politique d'allocation basé sur \emph{Multiple-Choice Knapsack Problem} (\textbf{MCKP}) pour la réprtition des noeud I/O aux app.

3. Il propose un service \emph{I/O forwarding} appelé \emph{GekkoFWD} qui implémente la politique \textbf{MCKP}. on-demand, au user-level, sans modif du code, simple à déployé, utilise sont propre fs.
Ce fs est enrichi pour perpètre différent déploiement??.

Le papier plaide pour :
- allocation dynamique des noeud I/O par rapport au type de calculs / patternes des application
- repartire des applications sur plusieur reources pour maximisé l'utilisation de la bande passante.
- Le changement de politique ce fait au démarrage ou au changement des applications (job) lancé. (pour déterminé le mailleur nb de I/O forwarding nodes)
- 

1. maximize la band passante en donnant des noeud de stockage au application qui vont mieux les utilisé. (et pas au applications qui accéde peut au données?).

propose : (user-level + portable + don't edit apps => solution utile)
- user-level i/o solution
- on demand
- applique différente politique d'allocation a l'execution (dynamique)
- changer le nombre de noeud I/O à l'execution sans perturbé l'app
-


Pour un ensemble de tâches à exécuter et un nombre fixe de de nœuds I/O il faut déterminé le nombre de noeud I/O forwarding qui maximise l'utilisation de la bande passante globale. Ce probléme dois étre considered as an optimization problem.

MCKP :
- Ce problem cherche à maximisé la bandpassat global en donnant plus de noeud I/O au applications qui en on le plus besoin.
- un problème d'optimisation
- derived from the 0-1 Knapsack
- items divide in $k$ classes of $N_i$ items
($N$ = ensemble des nombre d'items par classes)
$Items = {a; b; c; d; e; f; g}, k <- 2, Items/k = C <- {{a; b; c}; {d; e; f; g}},$
$card(C_1) = 3 = N_1, card(C_2) = 3 = N_2$
- $x_{ij} = 1$ ssi item $j$ is chosen in class $N_i$
- $w_i$ is the weight for the iéme items that represent number of I/O nodes.
- $W$ the total number of I/O nodes.
- $p_i$ is the bandwidth.

- problem is NP-hard
- la solution es obtenu par "Dynamic Programming" qui a une complexité en temps pseudo-polynomial ($O(W\sum^{k}_{i=1}N_i)$)


compute nodes must be derived by the number of I/O nodes.
on est limité par le nombre total de noeud I/O.

Il on fait des traces pour connaitre les access patterns (total data volum, nb processes makeing I/O requests...)

Il on fait des petit benchmark pour reproduire les patternes et cfaire des teste de perfs.

Pas besoin de profiling car on regarde juste les politique d'allocation.

Ils executé 189 patterns sur la machine MN4.
Ils on fait des groupe de 16 pour simulé chaque politique.

1 pattern = one app for this test.

Il on généré 10 000 groupe de 16 pattern qui prènne le même temps d'execution.
Jusque à 128 I/O forwarding nodes. (128 / 16app = 8 by app)
Number of compute nodes : median 256, minimum 88, maximum 512.

La band passante de chaque groupe est calculé par la somme de toutes les (WriteSize + ReadSize / runningTime)
\begin{equation*}aggregate\ BW=\sum_{a=1}^{16}\left(\frac{W_{a}+R_{a}}{runtime_{a}}\right) \tag{2}\end{equation*}

\section{Explication des algo}
% - an explanation of the algorithms;
Gekkofwd :
- politique d'allocation de noeud I/O.
- 

GekkoFS :
- Un system de fichier local au noeud de calcule
- Un \emph{burst-buffer} qui est un buffer qui ce met entre l'app et le FS pour amélioré les perfs. Ce gain de perf est obtenu car il augmente l'utilisation de la band passante (en permettant le stockage de données en même temps que le calcule) et permet d'attenué la charge sur le PFS quant il y à un peaks de demande.
- Fourni un system de nom qui est partagé entre tous les noeud de calcule. Ce mecanisme peut être remplacé par un PFS (grâce au mode d'extension GekkoFWD).
- 

Grâce à GekkoFWD, GekkoFS est utilisé comme un noeud intermédiaires entre le noeud de calcule et le noeud I/O.

Pour capturé les requêtes de l'app au PFS de façon transparent GekkoFS utilise intércepte les appéles system.

Une fois les requêtes capturé GekkoFWD transfére les requêtes à un seul serveur qui lui vas appliqué la politique d'allocation voulue.

Grâce à GekkoFS ils on donc ajouté un thread qui check si le mappage à changer.

Ce système transparent permet de d'ajouté des optiminsation comme la planification des demandes au niveau des fichiers donc il on ajouté la bibliothèque AGIOS dans GekkoFWD.

AGIOS :
- fourni plusieurs ordonanceur.

AGIOS permet à GekkoFWD d'utilisé différent ordonanceur.

Quand un noeud I/O resoive les requêtes il les envoi à AGIOS pour ordonancé le moment ou elle seront traité.

Une fois fini tous est envoyer au PFS.

\section{Le perfs}
% - commentary regarding the performance evaluation, if present;

Il on un gain global d'utilisation de la band passante jusqu'à 85\% par rapport à la politique statique de base.


Ils on fait les expérimentations sur la platform Grid 5000 (G5K) avec 2 cluster à Nancy : Grimoire (8 noeud) et Gros (124 noeud). % conf

5 différents app kernels + les micro-benchmark IOR :
1. S3D I/O Kernel
2. MADBench2
3. HACC-IO
4. S3aSim
5. NAS BT-IO

Ils on mesuré la band passant pour chaque applications et il confirm les résultat de FORGE. Ils ont fait ce test avec des nombre de noeud et des nombre de processus différent.

set of jobs composed of BT-C, BT-D, IOR-MPI, POSIX-L, MAD, and S3D to test dynamically changing the allocated I/O nodes

il svois que c'est pas ouf l'accés statique sauf pour S3D. Distribution non optimal. On vois que IOR-MPI pourrai être mailler avec plus d'un I/O forwarding nodes.

une tache de chaque app : HACC, IOR-MPI, SIM, IOR-MPI, IOR-MPI, POSIX-S, POSIX-L, BT-C, MAD, MAD, S3D, HACC, HACC, and BT-D. Figure 9 on voi les gains.



TODO: 5.2. Allocation Decisions


\section{Les autre publications}
% - information about other papers that reference it.

TODO: SECTION 6.

\section{Definition *}

La \emph{contention} : forte tentions sur un serveur (beaucoup de demandes).

\end{document}

% TODO: up to 8 pages

%L'article s'intéresse à différente politique d'accés au noeud de données en fonction des patternes d'accès au données par l'application.

% L'article propose une politique basé sur le problème du sac à dos (Knapsack problem) à choix multiple. Ce problem cherche à maximisé la bandpassat global en donnant plus de noeud I/O au applications qui en on le plus besoin.

%Le context de l'article est :
%- Les applications HPC font de plus en plus d'accès au données nommé (I/O)
%- Il y a pliens de type d'applications (accès hétérogénéité aux données, IA, bigData)
%- on augment le nombre de moeux de calculs pour avoir plus de performance mais le PFS ne peut pas scale pour autant de noeud. (je suppose une histoire de broadcast qui faut mettre en multicast?)
%- le \emph{I/O forwarding} cherche a corriger le problème en diminuent le nombre de machine qui accéde au PFS.

% NInt : noeud interface
% Ncal : noeud de calcule

% Le \emph{I/O forwarding} consiste à une groupe de noeud qui resoive les requétes de
% l'application et les transmet au system de fichier paralléle. Permet aussi de changer la form
% des requétes (Ex : au lieu de plusieurs petite requétes on en fait une seul grand).

% Le \emph{I/O forwarding} :
% - une technique très utilisé dans le HPC
% - Cette technique permet d'augmenté les performances
% - évite les accès direct au machine de stockage parallèle
% - NInt ce met entre les noeud de calcule et les noeud PFS
% - Ajouté des Nint permet de controlé les demand d'accès au données et d'appliqué des théchniques d'optimisation
% - c'est transparent pour les utilisateurs (app, FS...)
% - diminuent la contention à accès au serveurs de stockage de données.
% - try to répatition uniform ?
% - utilisé par des machine du top 500


% les théchniques d'optimisation :
% - la planification des demandes
% - l'agrégation des demandes.

% La technique habituelle est d'assigné statiquement un noeud de stockage à un noeud de calcule.
% Ces liens ce correspond pas tous le temps au transfére de données d'on l'appication à besoin.

% Le pb :
% - la connection entre les NInt et les Ncal est statique
% - Donc l'application peut utilisé qu'un nombre prédéfinis de noeud I/O est communique avec un seul NInt.
% - pas assé flexible
% - ça peut amené a de mauvaise allocation des resources (Yu et al. [8] Bez et al. [9])
% - La bande passant est déterminé par le fonctionnement des applications (plus ou moins de calcule avec plus ou moins d'accés au données...) ce qui fait qu'avec un nombre statique de noeud ou peut ce retouvé à avoir top de noeud I/O et donc une bande passant pas utilisé a sont pliens potenciel. (caractéristiques de la charge de travail / accès I/O).

% Pour les perfs du début ils on :
% - fait plusieurs lancement
% - exec sur MareNostrum 4 (MN4) supercomputer
% - avec différente configuration de forwarding
% - avec différent nombre de noeud I/O
% - pour voir différents patterns
% - utilise un outil FORGE (I/OForwarding Explorer) pour relancer le profiles I/O des app
% - il décrive les machines de MN4

% Politiques avec les quelle il ce sont comparer :
% - \emph{ZERO and ONE Policies}
% - \emph{STATIC Policy}
% - \emph{SIZE and PROCESS Policies}
% - \emph{ORACLE Policy}

% - pas facile de changer la strategies sans avoir un mauvé impacte car statique

% Il on en premier regardé que toutes les applications n'on pas les même perf avec le même nombre de I/O nodes. Il on vue que certain patternes on besoin de plus de I/O nodes et d'autre moins. Donc il on montré qu'une allocation statique c'est null.
